{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b1b9ff",
   "metadata": {},
   "source": [
    "PyTorch Lightning\n",
    "* Enforced standards\n",
    "* Rids code of boilerplate\n",
    "* Abstracts away a lot of extras - logging parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e34ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f58c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "validation_data_set = MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_dataloader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "        self.learning_rate = 0.5\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        data_batch = data_batch.flatten(1, -1)\n",
    "        return self.lin(data_batch)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data_batch, label_batch = batch\n",
    "        pred = self(data_batch)\n",
    "        loss = F.cross_entropy(pred, label_batch)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (pred.argmax(dim=1) == label_batch).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data_batch, label_batch = batch\n",
    "        pred = self(data_batch)\n",
    "        loss = F.cross_entropy(pred, label_batch)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (pred.argmax(dim=1) == label_batch).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81966505",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Neither `tensorboard` nor `tensorboardX` is available. Try `pip install`ing either.\nRequirement 'tensorboardX' not met. HINT: Try running `pip install -U 'tensorboardX'`\nRequirement 'tensorboard' not met. HINT: Try running `pip install -U 'tensorboard'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tensor_board_logger = \u001b[43mTensorBoardLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtb_logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\badmo\\anaconda3\\envs\\DL4begs\\Lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:96\u001b[39m, in \u001b[36mTensorBoardLogger.__init__\u001b[39m\u001b[34m(self, save_dir, name, version, log_graph, default_hp_metric, prefix, sub_dir, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     87\u001b[39m     save_dir: _PATH,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     **kwargs: Any,\n\u001b[32m     95\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_hp_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_hp_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43msub_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m log_graph \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TENSORBOARD_AVAILABLE:\n\u001b[32m    106\u001b[39m         rank_zero_warn(\n\u001b[32m    107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou set `TensorBoardLogger(log_graph=True)` but `tensorboard` is not available.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(_TENSORBOARD_AVAILABLE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\badmo\\anaconda3\\envs\\DL4begs\\Lib\\site-packages\\lightning_fabric\\loggers\\tensorboard.py:93\u001b[39m, in \u001b[36mTensorBoardLogger.__init__\u001b[39m\u001b[34m(self, root_dir, name, version, default_hp_metric, prefix, sub_dir, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     84\u001b[39m     root_dir: _PATH,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     **kwargs: Any,\n\u001b[32m     91\u001b[39m ):\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TENSORBOARD_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TENSORBOARDX_AVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m     94\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNeither `tensorboard` nor `tensorboardX` is available. Try `pip install`ing either.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(_TENSORBOARDX_AVAILABLE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(_TENSORBOARD_AVAILABLE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     97\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     98\u001b[39m     root_dir = os.fspath(root_dir)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Neither `tensorboard` nor `tensorboardX` is available. Try `pip install`ing either.\nRequirement 'tensorboardX' not met. HINT: Try running `pip install -U 'tensorboardX'`\nRequirement 'tensorboard' not met. HINT: Try running `pip install -U 'tensorboard'`"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "tensor_board_logger = TensorBoardLogger('tb_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\badmo\\anaconda3\\envs\\DL4begs\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name | Type   | Params | Mode \n",
      "----------------------------------------\n",
      "0 | lin  | Linear | 7.9 K  | train\n",
      "----------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\badmo\\anaconda3\\envs\\DL4begs\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:13<00:00, 71.51it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:13<00:00, 71.45it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "mnist_model = MNISTModel()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, logger=tensor_board_logger)\n",
    "\n",
    "trainer.fit(mnist_model, training_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(mnist_model, validation_dataloader)\n",
    "\n",
    "# Make a prediction on a single sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a sample from validation set\n",
    "sample_data, sample_label = next(iter(validation_dataloader))\n",
    "sample_image = sample_data[0]\n",
    "sample_true_label = sample_label[0]\n",
    "\n",
    "# Make prediction\n",
    "mnist_model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = mnist_model(sample_image.unsqueeze(0))\n",
    "    predicted_label = prediction.argmax(dim=1).item()\n",
    "\n",
    "# Display the image and prediction\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title(f'True: {sample_true_label}, Predicted: {predicted_label}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(10), F.softmax(prediction, dim=1).squeeze().numpy())\n",
    "plt.title('Prediction Probabilities')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4begs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
