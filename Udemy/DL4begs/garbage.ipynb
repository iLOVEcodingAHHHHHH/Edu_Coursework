{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebaa9078",
   "metadata": {},
   "source": [
    "Mnist AAN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH / 'mnist'\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Using a more reliable URL for MNIST dataset\n",
    "URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    print(f\"Downloading {FILENAME}...\")\n",
    "    response = requests.get(URL + FILENAME)\n",
    "    if response.status_code == 200:\n",
    "        (PATH / FILENAME).open('wb').write(response.content)\n",
    "        print(f\"Download completed. File size: {len(response.content)} bytes\")\n",
    "    else:\n",
    "        print(f\"Download failed with status code: {response.status_code}\")\n",
    "        print(f\"Response content: {response.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb90b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "File size: 17051982 bytes\n",
      "First bytes: b'\\x1f\\x8b\\x08\\x08|\\xa2+S\\x00\\x0bmnist.'\n"
     ]
    }
   ],
   "source": [
    "# Check if file exists and its size\n",
    "file_path = PATH / FILENAME\n",
    "print(f\"File exists: {file_path.exists()}\")\n",
    "if file_path.exists():\n",
    "    print(f\"File size: {file_path.stat().st_size} bytes\")\n",
    "    \n",
    "    # Check if it's a valid gzip file by reading the first few bytes\n",
    "    with open(file_path, 'rb') as f:\n",
    "        first_bytes = f.read(16)\n",
    "        print(f\"First bytes: {first_bytes}\")\n",
    "        \n",
    "    # If the file seems corrupted, delete it so it can be re-downloaded\n",
    "    if len(first_bytes) < 10 or not first_bytes.startswith(b'\\x1f\\x8b'):\n",
    "        print(\"File appears corrupted, deleting...\")\n",
    "        file_path.unlink()\n",
    "        print(\"File deleted. Re-run the download cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05da9393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ee465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap='grey')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f06e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: x_train=torch.Size([50000, 784]), y_train=torch.Size([50000])\n",
      "Data types: x_train=torch.float32, y_train=torch.int64\n",
      "Data ranges: x_train=[0.00, 0.00]\n",
      "Label range: y_train=[0, 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to tensors and normalize the data\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "# Normalize pixel values from 0-255 to 0-1\n",
    "x_train = x_train.float() / 255.0\n",
    "x_valid = x_valid.float() / 255.0\n",
    "\n",
    "# Debug information\n",
    "print(f\"Data shapes: x_train={x_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Data types: x_train={x_train.dtype}, y_train={y_train.dtype}\")\n",
    "print(f\"Data ranges: x_train=[{x_train.min():.2f}, {x_train.max():.2f}]\")\n",
    "print(f\"Label range: y_train=[{y_train.min()}, {y_train.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9a25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784) #Xavier init\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44de8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6d2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved loss function with better numerical stability\n",
    "def softmax(x):\n",
    "    exp_x = x.exp()  # Compute exp once for efficiency\n",
    "    return exp_x / exp_x.sum(-1).unsqueeze(-1)\n",
    "\n",
    "def cross_entropy_loss(pred, targets):\n",
    "    bs, out_features = pred.shape\n",
    "    one_hot_encoded_targets = torch.eye(out_features)[targets]\n",
    "    \n",
    "    # Use log_softmax for better numerical stability\n",
    "    log_probs = torch.log_softmax(pred, dim=-1)\n",
    "    return -(one_hot_encoded_targets * log_probs).sum() / bs\n",
    "\n",
    "loss_func = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a04dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(pred, yb):\n",
    "    pred_class = torch.argmax(pred, dim=1)\n",
    "    return (pred_class == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4dd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging predictions:\n",
      "Test labels: tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n",
      "Test labels dtype: torch.int64\n",
      "Raw predictions (first sample): tensor([-2.5524e-03, -2.8056e-03, -3.2510e-04, -1.9245e-03,  7.3500e-04,\n",
      "         2.1127e-03,  2.9468e-04,  1.0477e-03, -5.3469e-04,  3.9290e-05],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Predicted classes: tensor([5, 5, 1, 6, 5, 5, 5, 7, 5, 6])\n",
      "Predicted classes dtype: torch.int64\n",
      "Matches: tensor([ True, False, False, False, False, False, False, False, False, False])\n",
      "Manual accuracy: 0.10000000149011612\n",
      "Prediction range: [-0.003, 0.002]\n",
      "Are all predictions similar? 0.001310\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check what's going wrong with predictions\n",
    "test_batch = x_train[:10]\n",
    "test_labels = y_train[:10]\n",
    "test_pred = model(test_batch)\n",
    "\n",
    "print(\"Debugging predictions:\")\n",
    "print(f\"Test labels: {test_labels}\")\n",
    "print(f\"Test labels dtype: {test_labels.dtype}\")\n",
    "print(f\"Raw predictions (first sample): {test_pred[0]}\")\n",
    "print(f\"Predicted classes: {torch.argmax(test_pred, dim=1)}\")\n",
    "print(f\"Predicted classes dtype: {torch.argmax(test_pred, dim=1).dtype}\")\n",
    "\n",
    "# Check if there's a type mismatch\n",
    "pred_classes = torch.argmax(test_pred, dim=1)\n",
    "matches = (pred_classes == test_labels)\n",
    "print(f\"Matches: {matches}\")\n",
    "print(f\"Manual accuracy: {matches.float().mean()}\")\n",
    "\n",
    "# Check the range of predictions\n",
    "print(f\"Prediction range: [{test_pred.min():.3f}, {test_pred.max():.3f}]\")\n",
    "print(f\"Are all predictions similar? {torch.std(test_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d400ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 epochs, 782 batches per epoch, lr=1\n",
      "Total training samples: 50000\n",
      "\n",
      "=== Starting Epoch 1/2 ===\n",
      "  Epoch 1, Batch   0: Loss: 2.302949 Accuracy: 4.7%\n",
      "  Epoch 1, Batch 200: Loss: 2.315964 Accuracy: 12.5%\n",
      "  Epoch 1, Batch 400: Loss: 2.307671 Accuracy: 6.2%\n",
      "  Epoch 1, Batch 600: Loss: 2.291747 Accuracy: 17.2%\n",
      ">>> Epoch 1 COMPLETE - Avg Loss: 2.298300 Avg Accuracy: 11.9%\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Starting Epoch 2/2 ===\n",
      "  Epoch 2, Batch   0: Loss: 2.290680 Accuracy: 10.9%\n",
      "  Epoch 2, Batch 200: Loss: 2.303810 Accuracy: 14.1%\n",
      "  Epoch 2, Batch 400: Loss: 2.294398 Accuracy: 6.2%\n",
      "  Epoch 2, Batch 600: Loss: 2.277276 Accuracy: 43.8%\n",
      ">>> Epoch 2 COMPLETE - Avg Loss: 2.285282 Avg Accuracy: 15.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "bs = 64          # Batch size first\n",
    "epochs = 2\n",
    "lr = 1        # Reduced learning rate for more stable training\n",
    "n = x_train.shape[0]\n",
    "num_batches = n // bs + 1\n",
    "\n",
    "print(f\"Training with {epochs} epochs, {num_batches} batches per epoch, lr={lr}\")\n",
    "print(f\"Total training samples: {n}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n=== Starting Epoch {epoch+1}/{epochs} ===\")\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_i = i * bs\n",
    "        end_i = min(start_i + bs, n)  # Handle last batch properly\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        accuracy = accuracy_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            train_loss, train_accuracy = loss.item(), accuracy.item() * 100\n",
    "            print(f'  Epoch {epoch+1}, Batch {i:3d}: Loss: {train_loss:.6f} Accuracy: {train_accuracy:.1f}%')\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_accuracy = epoch_accuracy / num_batches * 100\n",
    "    print(f'>>> Epoch {epoch+1} COMPLETE - Avg Loss: {avg_loss:.6f} Avg Accuracy: {avg_accuracy:.1f}%')\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e55cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Results:\n",
      "Validation Loss: 2.278039\n",
      "Validation Accuracy: 16.4%\n",
      "\n",
      "Sample predictions vs true labels:\n",
      "Predicted: 3, True: 1\n",
      "Predicted: 3, True: 5\n",
      "Predicted: 0, True: 2\n",
      "Predicted: 0, True: 7\n",
      "Predicted: 0, True: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the model on validation data\n",
    "with torch.no_grad():\n",
    "    val_pred = model(x_valid)\n",
    "    val_accuracy = accuracy_func(val_pred, y_valid)\n",
    "    val_loss = loss_func(val_pred, y_valid)\n",
    "    \n",
    "    print(f\"Final Validation Results:\")\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.1f}%\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    sample_indices = torch.randint(0, len(x_valid), (5,))\n",
    "    sample_preds = torch.argmax(val_pred[sample_indices], dim=1)\n",
    "    sample_true = y_valid[sample_indices]\n",
    "    \n",
    "    print(f\"\\nSample predictions vs true labels:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Predicted: {sample_preds[i].item()}, True: {sample_true[i].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e496e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRYING WITH HIGHER LEARNING RATE ===\n",
      "New learning rate: 1.0\n",
      "Batch 0: Loss: 2.302949 Accuracy: 4.7%\n",
      "New prediction range: [-0.147, 0.185]\n"
     ]
    }
   ],
   "source": [
    "# Try with much higher learning rate\n",
    "print(\"=== RETRYING WITH HIGHER LEARNING RATE ===\")\n",
    "\n",
    "# Reset weights\n",
    "torch.manual_seed(0)\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# Much higher learning rate\n",
    "lr_new = 1.0  # 10x higher!\n",
    "epochs = 1\n",
    "bs = 64\n",
    "\n",
    "print(f\"New learning rate: {lr_new}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, 3000, bs):  # Just first 3000 samples for quick test\n",
    "        xb = x_train[i:i+bs]\n",
    "        yb = y_train[i:i+bs]\n",
    "        \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        accuracy = accuracy_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr_new\n",
    "            bias -= bias.grad * lr_new\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'Batch {i//bs}: Loss: {loss.item():.6f} Accuracy: {accuracy.item()*100:.1f}%')\n",
    "\n",
    "# Test final prediction range\n",
    "test_pred_new = model(x_train[:10])\n",
    "print(f\"New prediction range: [{test_pred_new.min():.3f}, {test_pred_new.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30961597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE FIX ===\n",
      "x_train range: [0.000, 0.004]\n",
      "y_train unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Initial weight range: [-0.038325, 0.034456]\n",
      "Using lr=0.5, epochs=3\n",
      "\n",
      "--- Epoch 1 ---\n",
      "  Batch  0: Loss=2.3026, Acc=9.4%\n",
      "  Batch 125: Loss=2.3063, Acc=7.8%\n",
      "Epoch 1 Summary: Loss=2.3017, Accuracy=11.0%\n",
      "  New best accuracy: 11.0%\n",
      "\n",
      "--- Epoch 2 ---\n",
      "  Batch  0: Loss=2.3023, Acc=12.5%\n",
      "  Batch 125: Loss=2.3049, Acc=7.8%\n",
      "Epoch 2 Summary: Loss=2.3004, Accuracy=11.4%\n",
      "  New best accuracy: 11.4%\n",
      "\n",
      "--- Epoch 3 ---\n",
      "  Batch  0: Loss=2.3009, Acc=14.1%\n",
      "  Batch 125: Loss=2.3034, Acc=7.8%\n",
      "Epoch 3 Summary: Loss=2.2990, Accuracy=11.6%\n",
      "  New best accuracy: 11.6%\n",
      "\n",
      "=== Final Test ===\n",
      "Test Accuracy: 12.4%\n",
      "Test Loss: 2.2981\n",
      "Prediction distribution: tensor([  0, 941,   0,  59])\n"
     ]
    }
   ],
   "source": [
    "# Complete diagnosis and fix\n",
    "print(\"=== COMPREHENSIVE FIX ===\")\n",
    "\n",
    "# 1. Check the data one more time\n",
    "print(f\"x_train range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"y_train unique values: {torch.unique(y_train)}\")\n",
    "\n",
    "# 2. Reset everything with better initialization\n",
    "torch.manual_seed(42)  # Different seed\n",
    "weights = torch.randn(784, 10) * 0.01  # Much smaller initialization\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "print(f\"Initial weight range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "\n",
    "# 3. Use a more reasonable learning rate\n",
    "lr_fixed = 0.5\n",
    "epochs = 3\n",
    "bs = 64\n",
    "\n",
    "print(f\"Using lr={lr_fixed}, epochs={epochs}\")\n",
    "\n",
    "# 4. Training with better monitoring\n",
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    correct_total = 0\n",
    "    loss_total = 0\n",
    "    batches_processed = 0\n",
    "    \n",
    "    for i in range(0, min(10000, len(x_train)), bs):  # First 10k samples\n",
    "        xb = x_train[i:i+bs]\n",
    "        yb = y_train[i:i+bs]\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        # Check for NaN\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss at batch {i//bs}!\")\n",
    "            break\n",
    "            \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_([weights, bias], max_norm=1.0)\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr_fixed\n",
    "            bias -= bias.grad * lr_fixed\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "        \n",
    "        # Track progress\n",
    "        with torch.no_grad():\n",
    "            pred_classes = torch.argmax(pred, dim=1)\n",
    "            correct = (pred_classes == yb).sum().item()\n",
    "            correct_total += correct\n",
    "            loss_total += loss.item()\n",
    "            batches_processed += 1\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            batch_acc = correct / len(yb) * 100\n",
    "            print(f\"  Batch {i//bs:2d}: Loss={loss.item():.4f}, Acc={batch_acc:.1f}%\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_acc = correct_total / min(10000, len(x_train)) * 100\n",
    "    epoch_loss = loss_total / batches_processed\n",
    "    print(f\"Epoch {epoch+1} Summary: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.1f}%\")\n",
    "    \n",
    "    if epoch_acc > best_accuracy:\n",
    "        best_accuracy = epoch_acc\n",
    "        print(f\"  New best accuracy: {best_accuracy:.1f}%\")\n",
    "\n",
    "# Final test\n",
    "print(f\"\\n=== Final Test ===\")\n",
    "with torch.no_grad():\n",
    "    test_pred = model(x_valid[:1000])  # Test on first 1000 validation samples\n",
    "    test_acc = accuracy_func(test_pred, y_valid[:1000]) * 100\n",
    "    test_loss = loss_func(test_pred, y_valid[:1000])\n",
    "    print(f\"Test Accuracy: {test_acc:.1f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    pred_classes = torch.argmax(test_pred, dim=1)\n",
    "    print(f\"Prediction distribution: {torch.bincount(pred_classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2106bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RELOADING DATA ===\n",
      "Raw data range: [0.0, 0.99609375]\n",
      "Normalized x_train range: [0.000, 0.004]\n",
      "y_train unique: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Weights range: [-0.038325, 0.034456]\n",
      "\n",
      "Quick test with properly normalized data:\n",
      "Step 0: Loss=2.3026, Acc=9.4%\n",
      "Step 1: Loss=2.3004, Acc=14.1%\n",
      "Step 2: Loss=2.2984, Acc=14.1%\n",
      "Step 3: Loss=2.2964, Acc=14.1%\n",
      "Step 4: Loss=2.2946, Acc=14.1%\n",
      "\n",
      "Prediction range after training: [-0.043, 0.048]\n"
     ]
    }
   ],
   "source": [
    "# EMERGENCY FIX: Reload and properly normalize data\n",
    "print(\"=== RELOADING DATA ===\")\n",
    "\n",
    "# Reload the original data\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    ((x_train_raw, y_train_raw), (x_valid_raw, y_valid_raw), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "print(f\"Raw data range: [{x_train_raw.min()}, {x_train_raw.max()}]\")\n",
    "\n",
    "# Convert to tensors and normalize properly\n",
    "x_train = torch.tensor(x_train_raw, dtype=torch.float32) / 255.0\n",
    "y_train = torch.tensor(y_train_raw, dtype=torch.long)\n",
    "x_valid = torch.tensor(x_valid_raw, dtype=torch.float32) / 255.0\n",
    "y_valid = torch.tensor(y_valid_raw, dtype=torch.long)\n",
    "\n",
    "print(f\"Normalized x_train range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"y_train unique: {torch.unique(y_train)}\")\n",
    "\n",
    "# Reset model with good initialization\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(784, 10) * 0.01\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "print(f\"Weights range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "\n",
    "# Quick training test\n",
    "lr = 0.3\n",
    "batch = x_train[:64]\n",
    "labels = y_train[:64]\n",
    "\n",
    "print(f\"\\nQuick test with properly normalized data:\")\n",
    "for step in range(5):\n",
    "    pred = model(batch)\n",
    "    loss = loss_func(pred, labels)\n",
    "    acc = accuracy_func(pred, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weights -= weights.grad * lr\n",
    "        bias -= bias.grad * lr\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    \n",
    "    print(f\"Step {step}: Loss={loss.item():.4f}, Acc={acc.item()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nPrediction range after training: [{pred.min():.3f}, {pred.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6a321db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THE REAL PROBLEM SOLVED ===\n",
      "Raw data range: [0.0, 0.99609375]\n",
      "Correct x_train range: [0.000, 0.996]\n",
      "y_train unique: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Weights range: [-0.392987, 0.368633]\n",
      "\n",
      "Training with PROPERLY normalized data:\n",
      "LR: 0.5, Epochs: 10\n",
      "\n",
      "--- Epoch 1 ---\n",
      "  Batch  0: Loss=3.0474, Acc=4.7%\n",
      "Epoch 1 Accuracy: 79.3%\n",
      "\n",
      "--- Epoch 2 ---\n",
      "  Batch  0: Loss=0.4356, Acc=90.6%\n",
      "Epoch 2 Accuracy: 89.5%\n",
      "\n",
      "--- Epoch 3 ---\n",
      "  Batch  0: Loss=0.3228, Acc=92.2%\n",
      "Epoch 3 Accuracy: 91.2%\n",
      "\n",
      "--- Epoch 4 ---\n",
      "  Batch  0: Loss=0.2696, Acc=93.8%\n",
      "Epoch 4 Accuracy: 92.0%\n",
      "\n",
      "--- Epoch 5 ---\n",
      "  Batch  0: Loss=0.2357, Acc=93.8%\n",
      "Epoch 5 Accuracy: 92.7%\n",
      "\n",
      "--- Epoch 6 ---\n",
      "  Batch  0: Loss=0.2120, Acc=95.3%\n",
      "Epoch 6 Accuracy: 93.0%\n",
      "\n",
      "--- Epoch 7 ---\n",
      "  Batch  0: Loss=0.1947, Acc=96.9%\n",
      "Epoch 7 Accuracy: 93.3%\n",
      "\n",
      "--- Epoch 8 ---\n",
      "  Batch  0: Loss=0.1814, Acc=96.9%\n",
      "Epoch 8 Accuracy: 93.8%\n",
      "\n",
      "--- Epoch 9 ---\n",
      "  Batch  0: Loss=0.1710, Acc=96.9%\n",
      "Epoch 9 Accuracy: 94.0%\n",
      "\n",
      "--- Epoch 10 ---\n",
      "  Batch  0: Loss=0.1625, Acc=96.9%\n",
      "Epoch 10 Accuracy: 94.1%\n",
      "\n",
      "=== FINAL VALIDATION ===\n",
      "Validation Accuracy: 89.7%\n",
      "Validation Loss: 0.3738\n",
      "Predictions per class: tensor([105, 111,  89,  92, 100,  99, 102, 105, 115,  82])\n",
      "Prediction range: [-17.514, 18.743]\n"
     ]
    }
   ],
   "source": [
    "# THE REAL FIX: Data is already normalized!\n",
    "print(\"=== THE REAL PROBLEM SOLVED ===\")\n",
    "\n",
    "# Reload the original data\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    ((x_train_raw, y_train_raw), (x_valid_raw, y_valid_raw), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "print(f\"Raw data range: [{x_train_raw.min()}, {x_train_raw.max()}]\")\n",
    "\n",
    "# DON'T divide by 255 - the data is already normalized!\n",
    "x_train = torch.tensor(x_train_raw, dtype=torch.float32)  # NO division!\n",
    "y_train = torch.tensor(y_train_raw, dtype=torch.long)\n",
    "x_valid = torch.tensor(x_valid_raw, dtype=torch.float32)  # NO division!\n",
    "y_valid = torch.tensor(y_valid_raw, dtype=torch.long)\n",
    "\n",
    "print(f\"Correct x_train range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"y_train unique: {torch.unique(y_train)}\")\n",
    "\n",
    "# Reset model with good initialization\n",
    "torch.manual_seed(123)\n",
    "weights = torch.randn(784, 10) * 0.1  # Slightly larger for better gradients\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "print(f\"Weights range: [{weights.min():.6f}, {weights.max():.6f}]\")\n",
    "\n",
    "# Now let's see proper training!\n",
    "lr = 0.5\n",
    "epochs = 10\n",
    "bs = 64\n",
    "\n",
    "print(f\"\\nTraining with PROPERLY normalized data:\")\n",
    "print(f\"LR: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    \n",
    "    for i in range(0, min(5000, len(x_train)), bs):  # First 5k samples\n",
    "        batch_x = x_train[i:i+bs]\n",
    "        batch_y = y_train[i:i+bs]\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(batch_x)\n",
    "        loss = loss_func(pred, batch_y)\n",
    "        acc = accuracy_func(pred, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "        \n",
    "        # Track stats\n",
    "        epoch_correct += (torch.argmax(pred, dim=1) == batch_y).sum().item()\n",
    "        epoch_total += len(batch_y)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  Batch {i//bs:2d}: Loss={loss.item():.4f}, Acc={acc.item()*100:.1f}%\")\n",
    "    \n",
    "    epoch_acc = epoch_correct / epoch_total * 100\n",
    "    print(f\"Epoch {epoch+1} Accuracy: {epoch_acc:.1f}%\")\n",
    "\n",
    "# Final validation test\n",
    "print(f\"\\n=== FINAL VALIDATION ===\")\n",
    "with torch.no_grad():\n",
    "    val_pred = model(x_valid[:1000])\n",
    "    val_acc = accuracy_func(val_pred, y_valid[:1000]) * 100\n",
    "    val_loss = loss_func(val_pred, y_valid[:1000])\n",
    "    print(f\"Validation Accuracy: {val_acc:.1f}%\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Check prediction distribution\n",
    "    pred_classes = torch.argmax(val_pred, dim=1)\n",
    "    print(f\"Predictions per class: {torch.bincount(pred_classes)}\")\n",
    "    print(f\"Prediction range: [{val_pred.min():.3f}, {val_pred.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34119f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4begs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
