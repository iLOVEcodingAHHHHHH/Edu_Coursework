{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188d4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b23a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "valid_data_set = MNIST(root='data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42352b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19866820c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data_set[0]\n",
    "print(label) # first index label of mnist\n",
    "plt.imshow(image.float().reshape(28, 28), cmap='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eeaacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader -- shuffle and loop over batches\n",
    "batch_size = 64\n",
    "train_DL = DataLoader(train_data_set, batch_size=batch_size, shuffle=True)\n",
    "valid_DL = DataLoader(valid_data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(28**2, 10)\n",
    "\n",
    "#single layer, add and forward more nn.Linear for more layers, will need matrix size adjustments\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        data_batch = data_batch.flatten(1, -1) # (BS, 1, 28, 28) -> (BS, 784)\n",
    "        return self.lin(data_batch)\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a42e16b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTModel(\n",
      "  (lin): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7078ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "# loss func - torch includes softmax activation\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# accuracy\n",
    "def accuracy_func(pred, label_batch):\n",
    "    pred_class = torch.argmax(pred, dim=1)\n",
    "    return (pred_class == label_batch).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ced06821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    for batch_idx, (data_batch, label_batch) in enumerate(dataloader):\n",
    "        pred = model(data_batch)\n",
    "        loss = loss_func(pred, label_batch)\n",
    "\n",
    "        loss.backward() # autograd\n",
    "        optimizer.step() # updates params using optimizer (SGD/Adam)\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'loss: {loss.item()}, acc: {accuracy_func(pred, label_batch)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37b75655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_batch, label_batch in dataloader:\n",
    "            pred = model(data_batch)\n",
    "            loss = loss_func(pred, label_batch)\n",
    "            accuracy = accuracy_func(pred, label_batch)\n",
    "\n",
    "            test_loss, test_accuracy = loss.item(), accuracy.item() * 100\n",
    "\n",
    "            print(f'test:\\n loss: {test_loss:6f}\\nacc: {test_accuracy:0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69632e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2397663593292236, acc: 26.5625%\n",
      "loss: 0.40102872252464294, acc: 92.1875%\n",
      "loss: 0.3468601107597351, acc: 93.75%\n",
      "loss: 0.3559693694114685, acc: 85.9375%\n",
      "loss: 0.21372941136360168, acc: 93.75%\n",
      "loss: 0.46703192591667175, acc: 85.9375%\n",
      "loss: 0.3001147508621216, acc: 87.5%\n",
      "loss: 0.273787260055542, acc: 89.0625%\n",
      "loss: 0.42256641387939453, acc: 92.1875%\n",
      "loss: 0.26122790575027466, acc: 92.1875%\n",
      "test:\n",
      " loss: 0.356205\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.386538\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.199156\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.270566\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.325029\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.295299\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.313497\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.306778\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.277135\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.187354\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.316545\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.556691\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.301544\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.224562\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.305750\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.197813\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.289273\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.298117\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.484432\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.641038\n",
      "acc: 81.2%\n",
      "test:\n",
      " loss: 0.306463\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.451247\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.314907\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.295548\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.205892\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.304599\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.257219\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.273612\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.267508\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.300551\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.252894\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.399767\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.417491\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.267189\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.233560\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.153549\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.261071\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.330698\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.298030\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.379430\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.435284\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.357642\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.178412\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.423225\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.599498\n",
      "acc: 81.2%\n",
      "test:\n",
      " loss: 0.388549\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.391957\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.319219\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.675579\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.434442\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.179333\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.374283\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.352212\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.333960\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.322845\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.279877\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.439200\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.607651\n",
      "acc: 79.7%\n",
      "test:\n",
      " loss: 0.579274\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.429300\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.136787\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.465368\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.409454\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.295117\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.298904\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.279579\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.187084\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.366482\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.439451\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.381935\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.357477\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.222476\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.558869\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.431120\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.375630\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.206341\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.357092\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.233572\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.340872\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.403309\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.341608\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.243817\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.302366\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.221880\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.439864\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.348949\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.507961\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.193571\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.170443\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.313791\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.325982\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.488821\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.549419\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.563814\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.355947\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.203870\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.249014\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.457339\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.392437\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.302161\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.256800\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.216899\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.374052\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.339626\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.394912\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.398505\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.164055\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.166585\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.278764\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.602830\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.211328\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.180146\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.480173\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.354238\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.259978\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.243095\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.513031\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.567901\n",
      "acc: 82.8%\n",
      "test:\n",
      " loss: 0.239028\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.342116\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.413283\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.378227\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.268922\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.314408\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.401349\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.454310\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.221376\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.262667\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.497716\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.220694\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.500612\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.240315\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.597034\n",
      "acc: 78.1%\n",
      "test:\n",
      " loss: 0.238634\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.265850\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.275949\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.360977\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.259725\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.198556\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.416258\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.498315\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.366386\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.188589\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.347766\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.282373\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.327146\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.248964\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.362864\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.185416\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.411682\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.407838\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.398467\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.305977\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.385500\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.239257\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.321155\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.401057\n",
      "acc: 93.8%\n",
      "loss: 0.23573671281337738, acc: 95.3125%\n",
      "loss: 0.1729319989681244, acc: 95.3125%\n",
      "loss: 0.18492218852043152, acc: 95.3125%\n",
      "loss: 0.41936540603637695, acc: 90.625%\n",
      "loss: 0.27244898676872253, acc: 93.75%\n",
      "loss: 0.2716714143753052, acc: 90.625%\n",
      "loss: 0.13020595908164978, acc: 96.875%\n",
      "loss: 0.30478718876838684, acc: 90.625%\n",
      "loss: 0.16894620656967163, acc: 93.75%\n",
      "loss: 0.1907910853624344, acc: 92.1875%\n",
      "test:\n",
      " loss: 0.367918\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.447969\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.309367\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.327388\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.230538\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.202749\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.359094\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.112584\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.470301\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.180702\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.292550\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.348793\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.218675\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.226419\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.275714\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.144164\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.153455\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.350135\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.139278\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.433244\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.578189\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.484470\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.254915\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.257923\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.236208\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.298968\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.243289\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.203128\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.243304\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.563394\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.422191\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.383107\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.319844\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.259432\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.429854\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.117676\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.151532\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.243755\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.458018\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.504767\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.377338\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.276350\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.228245\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.101985\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.184921\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.349127\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.238915\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.212961\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.162624\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.160414\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.361560\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.355312\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.156424\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.113548\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.209725\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.136008\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.291887\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.354949\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.133516\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.337868\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.384376\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.287649\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.448058\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.228020\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.464950\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.163558\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.376934\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.121583\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.369751\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.216526\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.303222\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.304534\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.234431\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.367689\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.191054\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.165384\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.204153\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.164001\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.229473\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.372757\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.211675\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.087145\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.240701\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.345771\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.200881\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.194929\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.150254\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.287542\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.294726\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.263484\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.582348\n",
      "acc: 82.8%\n",
      "test:\n",
      " loss: 0.220989\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.267207\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.251791\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.389973\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.142963\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.239360\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.242893\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.307253\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.256322\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.607034\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.267747\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.285217\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.470230\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.168999\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.285752\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.124721\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.511526\n",
      "acc: 85.9%\n",
      "test:\n",
      " loss: 0.482558\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.320333\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.444461\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.359384\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.375231\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.172303\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.213922\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.502011\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.211278\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.154076\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.394365\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.349277\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.352162\n",
      "acc: 89.1%\n",
      "test:\n",
      " loss: 0.237874\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.388004\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.178068\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.405730\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.321498\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.317086\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.353333\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.285940\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.402520\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.098685\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.278943\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.191751\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.235265\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.396037\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.345832\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.161510\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.256295\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.338334\n",
      "acc: 90.6%\n",
      "test:\n",
      " loss: 0.270704\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.262130\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.383274\n",
      "acc: 87.5%\n",
      "test:\n",
      " loss: 0.487308\n",
      "acc: 84.4%\n",
      "test:\n",
      " loss: 0.243353\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.158922\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.244900\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.216397\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.126700\n",
      "acc: 96.9%\n",
      "test:\n",
      " loss: 0.171734\n",
      "acc: 95.3%\n",
      "test:\n",
      " loss: 0.191530\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.691616\n",
      "acc: 81.2%\n",
      "test:\n",
      " loss: 0.342635\n",
      "acc: 92.2%\n",
      "test:\n",
      " loss: 0.285303\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.241872\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.158860\n",
      "acc: 98.4%\n",
      "test:\n",
      " loss: 0.167525\n",
      "acc: 93.8%\n",
      "test:\n",
      " loss: 0.232746\n",
      "acc: 87.5%\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(train_DL, model, loss_func, optimizer)\n",
    "    test(valid_DL, model, loss_func)\n",
    "\n",
    "print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167a88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4begs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
